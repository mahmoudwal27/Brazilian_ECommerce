# -*- coding: utf-8 -*-
"""Brazilian ECommerce.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wk7IyeqzIDDKkXLYMACAQ4PU-heOUL3Z

# step 1: importing data
"""

import pandas as pd

file_path = "/content/drive/MyDrive/Google Data Analysis Tech challenge/olist_customers_dataset.csv"
df = pd.read_csv(file_path)
file_path = "/content/drive/MyDrive/Google Data Analysis Tech challenge/olist_geolocation_dataset.csv"
df1 = pd.read_csv(file_path)
file_path = "/content/drive/MyDrive/Google Data Analysis Tech challenge/olist_order_items_dataset.csv"
df2 = pd.read_csv(file_path)
file_path = "/content/drive/MyDrive/Google Data Analysis Tech challenge/olist_order_payments_dataset.csv"
df3 = pd.read_csv(file_path)
file_path = "/content/drive/MyDrive/Google Data Analysis Tech challenge/olist_order_reviews_dataset.csv"
df4 = pd.read_csv(file_path)
file_path = "/content/drive/MyDrive/Google Data Analysis Tech challenge/olist_orders_dataset.csv"
df5 = pd.read_csv(file_path)
file_path = "/content/drive/MyDrive/Google Data Analysis Tech challenge/olist_products_dataset.csv"
df6 = pd.read_csv(file_path)
file_path = "/content/drive/MyDrive/Google Data Analysis Tech challenge/olist_sellers_dataset.csv"
df7 = pd.read_csv(file_path)

"""# step 2: E.D.A.

### olist_customers_dataset (df)
"""

df.head(10)

df["customer_zip_code_prefix"] = df["customer_zip_code_prefix"].astype(str).str.zfill(5)

df.info()

df.describe()

"""### olist_geolocation_dataset (df1)"""

df1.head(10)

df1["geolocation_zip_code_prefix"] = df1["geolocation_zip_code_prefix"].astype(str).str.zfill(5)

df1.info()

"""### olist_order_items_dataset (df2)"""

df2.head(10)

df2.info()

df2["shipping_limit_date"] = pd.to_datetime(df2["shipping_limit_date"])

"""### olist_order_payments_dataset (df3)"""

df3.head(10)

df3.info()

"""### olist_order_reviews_dataset (df4)"""

df4.head(10)

df4.info()

df4["review_creation_date"] = pd.to_datetime(df4["review_creation_date"])
df4["review_answer_timestamp"] = pd.to_datetime(df4["review_answer_timestamp"])

"""### olist_orders_dataset (df5)"""

df5.head(10)

df5.info()

df5["order_purchase_timestamp"] = pd.to_datetime(df5["order_purchase_timestamp"])
df5["order_approved_at"] = pd.to_datetime(df5["order_approved_at"])
df5["order_delivered_carrier_date"] = pd.to_datetime(df5["order_delivered_carrier_date"])
df5["order_delivered_customer_date"] = pd.to_datetime(df5["order_delivered_customer_date"])
df5["order_estimated_delivery_date"] = pd.to_datetime(df5["order_estimated_delivery_date"])

"""### olist_products_dataset (df6)"""

df6.head(10)

df6.info()

df6["product_name_lenght"] = df6["product_name_lenght"].astype(int)
df6["product_description_lenght"] = df6["product_description_lenght"].astype(int)
df6["product_photos_qty"] = df6["product_photos_qty"].astype(int)
df6["product_weight_g"] = df6["product_weight_g"].astype(int)
df6["product_length_cm"] = df6["product_length_cm"].astype(int)
df6["product_height_cm"] = df6["product_height_cm"].astype(int)
df6["product_width_cm"] = df6["product_width_cm"].astype(int)

"""### olist_sellers_dataset (df7)"""

df7.head(10)

df7.info()

df7["seller_zip_code_prefix"] = df7["seller_zip_code_prefix"].astype(str).str.zfill(5)

"""# step 3: data cleaning

## step 3.1: deticting nulls
"""

df.isnull().sum()

df1.isnull().sum()

df2.isnull().sum()

df3.isnull().sum()

df4.isnull().sum()

df5.isnull().sum()

df6.isnull().sum()

df7.isnull().sum()

"""## step 3.2: deticting duplecits

"""

df.duplicated().sum()

df1.duplicated().sum()

df2.duplicated().sum()

df3.duplicated().sum()

df4.duplicated().sum()

df5.duplicated().sum()

df6.duplicated().sum()

df7.duplicated().sum()

"""## step 3.3: nulls"""

df6.dropna(subset=["product_category_name"], inplace=True)
df6.dropna(subset=["product_weight_g"], inplace=True)
df6.dropna(subset=["product_length_cm"], inplace=True)
df6.dropna(subset=["product_height_cm"], inplace=True)
df6.dropna(subset=["product_width_cm"], inplace=True)
df6.dropna(subset=["product_description_lenght"], inplace=True)
df6.dropna(subset=["product_photos_qty"], inplace=True)
df6.dropna(subset=["product_name_lenght"], inplace=True)

"""## step 3.4: Removing Duplicates"""

df1.drop_duplicates(subset=["geolocation_zip_code_prefix"], keep="first", inplace=True)

df1.duplicated().sum()

customers_pbi = df[["customer_id", "customer_unique_id", "customer_zip_code_prefix"]]
geolocation_pbi = df1[["geolocation_zip_code_prefix", "geolocation_lat", "geolocation_lng", "geolocation_city", "geolocation_state"]]
order_items_pbi = df2[["order_id", "order_item_id", "product_id", "seller_id", "shipping_limit_date", "price", "freight_value"]]
order_payments_pbi = df3[["order_id", "payment_sequential", "payment_type", "payment_installments", "payment_value"]]
order_reviews_pbi = df4[["review_id", "order_id", "review_score", "review_creation_date", "review_answer_timestamp"]]
orders_pbi = df5[["order_id", "customer_id", "order_status", "order_purchase_timestamp", "order_approved_at", "order_delivered_carrier_date", "order_delivered_customer_date", "order_estimated_delivery_date"]]
products_pbi = df6[["product_id", "product_category_name", "product_name_lenght", "product_description_lenght", "product_photos_qty", "product_weight_g", "product_length_cm", "product_height_cm", "product_width_cm"]]
sellers_pbi = df7[["seller_id", "seller_zip_code_prefix", "seller_city", "seller_state"]]

# Save prepared data for Power BI
customers_pbi.to_csv("/content/drive/My Drive/customers_pbi.csv", index=False)
geolocation_pbi.to_csv("/content/drive/My Drive/geolocation_pbi.csv", index=False)
order_items_pbi.to_csv("/content/drive/My Drive/order_items_pbi.csv", index=False)
order_payments_pbi.to_csv("/content/drive/My Drive/order_payments_pbi.csv", index=False)
order_reviews_pbi.to_csv("/content/drive/My Drive/order_reviews_pbi.csv", index=False)
orders_pbi.to_csv("/content/drive/My Drive/orders_pbi.csv", index=False)
products_pbi.to_csv("/content/drive/My Drive/products_pbi.csv", index=False)
sellers_pbi.to_csv("/content/drive/My Drive/sellers_pbi.csv", index=False)